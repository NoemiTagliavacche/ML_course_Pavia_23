{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf258xT0XIwV"
      },
      "source": [
        "# Training a Jet Tagging with **DNN** \n",
        "\n",
        "---\n",
        "In this notebook, we perform a Jet identification task using a multiclass classifier based on a \n",
        "Dense Neural Network (DNN), also called multi-layer perceptron (MLP). The problem consists on identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
        "based on set of physics-motivated high-level features.\n",
        "\n",
        "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf \n",
        "\n",
        "For details on the dataset, see Notebook1\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4OMAZgtyXIwY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lbB-J3hXIwb"
      },
      "source": [
        "# Preparation of the training and validation samples\n",
        "\n",
        "---\n",
        "In order to import the dataset, we now\n",
        "- clone the dataset repository (to import the data in Colab)\n",
        "- load the h5 files in the data/ repository\n",
        "- extract the data we need: a target and jetImage \n",
        "\n",
        "To type shell commands, we start the command line with !\n",
        "\n",
        "**nb, if you are running locally and you have already downloaded the datasets you can skip the cell below and, if needed, change the paths later to point to the folder with your previous download of the datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jWjxFaRPXIwb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  333M    0  333M    0     0  9851k      0 --:--:--  0:00:34 --:--:-- 11.5Mk      0 --:--:--  0:00:33 --:--:-- 10.2M\n",
            "x Data-MLtutorial/\n",
            "x Data-MLtutorial/JetDataset/\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_40000_50000.h5\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_70000_80000.h5\n",
            "x Data-MLtutorial/JetDataset/jetImage_7_100p_80000_90000.h5\n",
            "jetImage_7_100p_0_10000.h5     jetImage_7_100p_50000_60000.h5\n",
            "jetImage_7_100p_10000_20000.h5 jetImage_7_100p_60000_70000.h5\n",
            "jetImage_7_100p_30000_40000.h5 jetImage_7_100p_70000_80000.h5\n",
            "jetImage_7_100p_40000_50000.h5 jetImage_7_100p_80000_90000.h5\n"
          ]
        }
      ],
      "source": [
        "! curl https://cernbox.cern.ch/s/6Ec5pGFEpFWeH6S/download -o Data-MLtutorial.tar.gz\n",
        "! tar -xvzf Data-MLtutorial.tar.gz \n",
        "! ls Data-MLtutorial/JetDataset/\n",
        "! rm Data-MLtutorial.tar.gz "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cCGhrKdwXIwc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5\n",
            "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5\n",
            "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5\n",
            "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5\n",
            "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5\n",
            "(50000, 5) (50000, 16)\n"
          ]
        }
      ],
      "source": [
        "target = np.array([])\n",
        "features = np.array([])\n",
        "# we cannot load all data on Colab. So we just take a few files\n",
        "datafiles = ['Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
        "# if you are running locallt, you can use the full dataset doing\n",
        "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
        "for fileIN in datafiles:\n",
        "    print(\"Appending %s\" %fileIN)\n",
        "    f = h5py.File(fileIN)\n",
        "    myFeatures = np.array(f.get(\"jets\")[:,[12, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 52]])\n",
        "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
        "    features = np.concatenate([features, myFeatures], axis=0) if features.size else myFeatures\n",
        "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
        "    f.close()\n",
        "print(target.shape, features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a333RYPXIwe"
      },
      "source": [
        "The dataset consists of 50000 jets, each represented by 16 features\n",
        "\n",
        "---\n",
        "\n",
        "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZBqFs1eBXIwf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(33500, 16) (16500, 16) (33500, 5) (16500, 5)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.33)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "del features, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkNz5UAhXIwg"
      },
      "source": [
        "# DNN model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tTSDOiEHXIwh"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# keras imports\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Input, Dropout, Flatten, Activation\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_model\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "# keras imports\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAl0DZTxXIwi"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train.shape[1]\n",
        "dropoutRate = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l492G8BXIwj"
      },
      "outputs": [],
      "source": [
        "####\n",
        "inputArray = Input(shape=(input_shape,)) # Input function from keras= is used to instantiate a Keras tensor\n",
        "#tf.keras.Input(shape=None,batch_size=None,name=None,dtype=None,sparse=None,tensor=None,ragged=None,type_spec=None,**kwargs)\n",
        "#shape=A shape tuple (integers), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors. \n",
        "\n",
        "######FIRST LAYER\n",
        "x = Dense(40, activation='relu')(inputArray) #Dense=Just your regular densely-connected NN layer\n",
        "#Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, \n",
        "# kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
        "# units=Positive integer, dimensionality of the output space. (40 in our case)\n",
        "#activation=Activation function to use.\n",
        "#inputArray to specify the dimension of the input array\n",
        "#in questo modello entro con array di shape (input_shape,) e ottengo come output array di shape (units=40,0)\n",
        "x = Dropout(dropoutRate)(x) #Dropout=Applies Dropout to the input (x). \n",
        "#The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
        "# Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
        "\n",
        "########SECOND LAYER\n",
        "x = Dense(20)(x)\n",
        "x = Activation('relu')(x)    #perchè qui metto attivation separato da dense?\n",
        "x = Dropout(dropoutRate)(x)\n",
        "#\n",
        "x = Dense(10, activation='relu')(x)\n",
        "x = Dropout(dropoutRate)(x)\n",
        "#\n",
        "x = Dense(5, activation='relu')(x)\n",
        "#\n",
        "output = Dense(5, activation='softmax')(x)\n",
        "####\n",
        "model = Model(inputs=inputArray, outputs=output) # model=A model grouping layers into an object with training/inference features.\n",
        "# inputs=The input(s) of the model: a keras.Input object or a combination of keras.Input objects in a dict,\n",
        "# outputs = The output(s) of the model: a tensor that originated from keras.Input objects or a combination of such tensors in a dict\n",
        "#instantiate a Model= where you start from Input, you chain layer calls to specify the model's forward pass, and finally you create your model from inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu8rRUkhXIwj"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam') #compile=Configures the model for training.\n",
        "\n",
        "model.summary() #Prints a string summary of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HfKWoOtXIwk"
      },
      "source": [
        "We now train the model with these settings:\n",
        "\n",
        "- the **batch size** is a hyperparameter of gradient descent that controls the number of training samples to work through before the model internal parameters are updated\n",
        "    - batch size = 1 results in fast computation but noisy training that is slow to converge\n",
        "    - batch size = dataset size results in slow computation but faster convergence)\n",
        "\n",
        "- the **number of epochs** controls the number of complete passes through the full training dataset -- at each epoch gradients are computed for each of the mini batches and model internal parameters are updated.\n",
        "\n",
        "- the **callbacks** are algorithms used to optimize the training (full list [here](https://keras.io/api/callbacks/)):\n",
        "    - *EarlyStopping*: stop training when a monitored metric (`monitor`) has stopped improving in the last N epochs (`patience`)\n",
        "    - *ReduceLROnPlateau*: reduce learning rate when a metric (`monitor`) has stopped improving in the last N epochs (`patience`)\n",
        "    - *TerminateOnNaN*: terminates training when a NaN loss is encountered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzO-lyLEXIwk"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "n_epochs = 50\n",
        "\n",
        "# train \n",
        "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
        "                validation_data=(X_val, y_val),\n",
        "                callbacks = [\n",
        "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
        "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
        "                TerminateOnNaN()])  #fit=Trains the model for a fixed number of epochs (dataset iterations)\n",
        "\n",
        "#verbose??????"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "044bCLqVXIwl"
      },
      "outputs": [],
      "source": [
        "# plot training history\n",
        "plt.plot(history.history['loss'])   #to plot the loss function \n",
        "plt.plot(history.history['val_loss']) #to plot the loss function on the validation set vs epoch\n",
        "plt.yscale('log')\n",
        "plt.title('Training History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oESSmNLxXIwm"
      },
      "source": [
        "# Building the ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_AROD6SXIwm"
      },
      "outputs": [],
      "source": [
        "labels = ['gluon', 'quark', 'W', 'Z', 'top']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjKT7EjUXIwn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "predict_val = model.predict(X_val)\n",
        "df = pd.DataFrame()\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "auc1 = {}\n",
        "\n",
        "plt.figure()\n",
        "for i, label in enumerate(labels):\n",
        "        df[label] = y_val[:,i]\n",
        "        df[label + '_pred'] = predict_val[:,i]\n",
        "\n",
        "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred']) #per fare la curva ROC\n",
        "\n",
        "        auc1[label] = auc(fpr[label], tpr[label]) #calcolo AUC che mi dice quanto è buono il mio modello (calcola l'area sotto la ROC curve)\n",
        "\n",
        "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.)) \n",
        "plt.semilogy()\n",
        "plt.xlabel(\"sig. efficiency\")\n",
        "plt.ylabel(\"bkg. mistag rate\")\n",
        "plt.ylim(0.000001,1)\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzbQ-d0RKVmV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Notebook2_JetID_DNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
